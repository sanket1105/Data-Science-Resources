{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this you split the training dataset into various chunks say K chunks.\n",
    "\n",
    "You use k-1 chunks for training purpose and the last chunk for testing.\n",
    "\n",
    "The model performs in such a way that each chunk created is used for testing and in the end, average is taken of all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets, svm\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = load_digits()\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(dg.data, dg.target,test_size=0.2,random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9361111111111111"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model_log = LogisticRegression(solver='liblinear',multi_class='ovr')\n",
    "model_log.fit(X_train,y_train)\n",
    "model_log.score(X_test,y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9805555555555555"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "svm.fit(X_train,y_train)\n",
    "svm.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9555555555555556"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=40)\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFold(n_splits=3, random_state=None, shuffle=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3)\n",
    "kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 5 6 7 8] [0 1 2]\n",
      "[0 1 2 6 7 8] [3 4 5]\n",
      "[0 1 2 3 4 5] [6 7 8]\n"
     ]
    }
   ],
   "source": [
    "## lets see how kfold works on simple example\n",
    "\n",
    "for train_index, test_index in kf.split([1,2,3,4,5,6,7,8,9]):\n",
    "    print(train_index, test_index)\n",
    "\n",
    "## see in the test_index, all the points are covered \n",
    "# \n",
    "# see also by chnaging the n_splits to 2   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model for getting the accuracy\n",
    "\n",
    "def get_score(model,X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use stratifiedKfold when you want the same split to be taken into consideration when you are want to know about accuracy of diff. models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three commonly used variations are as follows:\n",
    "\n",
    "\n",
    "Train/Test Split: Taken to one extreme, k may be set to 2 (not 1) such that a single train/test split is created to evaluate the model.\n",
    "\n",
    "\n",
    "LOOCV: Taken to another extreme, k may be set to the total number of observations in the dataset such that each observation is given a chance to be the held out of the dataset. This is called leave-one-out cross-validation, or LOOCV for short.\n",
    "\n",
    "\n",
    "Stratified: The splitting of data into folds may be governed by criteria such as ensuring that each fold has the same proportion of observations with a given categorical value, such as the class outcome value. This is called stratified cross-validation.\n",
    "\n",
    "\n",
    "Repeated: This is where the k-fold cross-validation procedure is repeated n times, where importantly, the data sample is shuffled prior to each repetition, which results in a different split of the sample.\n",
    "\n",
    "\n",
    "Nested: This is where k-fold cross-validation is performed within each fold of cross-validation, often to perform hyperparameter tuning during model evaluation. This is called nested cross-validation or double cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "folds = StratifiedKFold(n_splits=3)\n",
    "\n",
    "scores_logistic =[]\n",
    "scores_svm = []\n",
    "scores_rf = []\n",
    "\n",
    "for train_index, test_index in folds.split(dg.data, dg.target):\n",
    "    X_train, X_test, y_train, y_test = dg.data[train_index,], dg.data[test_index], \\\n",
    "                                        dg.target[train_index], dg.target[test_index]\n",
    "                            \n",
    "    scores_logistic.append(get_score(model_log,X_train, X_test, y_train, y_test)) \n",
    "    scores_svm.append(get_score(svm,X_train, X_test, y_train, y_test)) \n",
    "    scores_rf.append(get_score(rf,X_train, X_test, y_train, y_test)) \n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8948247078464107, 0.9532554257095158, 0.9098497495826378]\n",
      "[0.9649415692821369, 0.9799666110183639, 0.9649415692821369]\n",
      "[0.9382303839732888, 0.9432387312186978, 0.9315525876460768]\n"
     ]
    }
   ],
   "source": [
    "print(scores_logistic)\n",
    "print(scores_svm)\n",
    "print(scores_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.89482471 0.95325543 0.90984975]\n",
      "[0.96494157 0.97996661 0.96494157]\n",
      "[0.92320534 0.94824708 0.92153589]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "## logistics reg\n",
    "print(cross_val_score(model_log,dg.data,dg.target,cv =3))\n",
    "\n",
    "## svm\n",
    "print(cross_val_score(svm,dg.data,dg.target,cv =3))\n",
    "\n",
    "## random forest\n",
    "print(cross_val_score(rf,dg.data,dg.target,cv =3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10   0.9254096834264433\n",
      "20   0.9404469273743017\n",
      "30   0.9404531346989445\n",
      "50   0.9465735567970205\n",
      "80   0.9471135940409683\n",
      "100   0.9488050900062073\n",
      "150   0.9465735567970205\n"
     ]
    }
   ],
   "source": [
    "## lets see parametere tuning in RF\n",
    "\n",
    "n = [10,20,30,50,80,100,150]\n",
    "\n",
    "for i in n:\n",
    "    scores2 = 0\n",
    "    scores2 = cross_val_score(RandomForestClassifier(n_estimators = i),dg.data,dg.target,cv =10)\n",
    "    print(i,\" \", np.average(scores2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6b6e8e5f654a8e9bb9e69623dd15cf09cd0bf1a8f120d3fdbf2d2c9cdde81e19"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('gpu': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
